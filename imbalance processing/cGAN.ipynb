{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/funway/Countdown/blob/master/imbalance%20processing/cGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv4skFSheBzA"
      },
      "source": [
        "# ä½¿ç”¨ cGAN å¯¹è®­ç»ƒé›†è¿›è¡Œè¿‡é‡‡æ ·\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqqi2ITUeOmB"
      },
      "source": [
        "## Google Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzbTh_MiPCpH",
        "outputId": "a0b25dc9-6a6f-4d70-bf3a-276b50cebf57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR21AFUgeYXb"
      },
      "source": [
        "## Modules import & Globals setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKNH9UjKeZmU"
      },
      "outputs": [],
      "source": [
        "### Modules ###\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "### Globals ###\n",
        "\n",
        "## Label ç‰¹å¾çš„æ•°å€¼åŒ–ç¼–ç \n",
        "label_mapping = {\n",
        "    \"Benign\": 0,\n",
        "    \"Bot\": 1,\n",
        "    \"Brute Force -Web\": 2,\n",
        "    \"Brute Force -XSS\": 3,\n",
        "    \"DDOS attack-HOIC\": 4,\n",
        "    \"DDOS attack-LOIC-UDP\": 5,\n",
        "    \"DDoS attacks-LOIC-HTTP\": 6,\n",
        "    \"DoS attacks-GoldenEye\": 7,\n",
        "    \"DoS attacks-Hulk\": 8,\n",
        "    \"DoS attacks-SlowHTTPTest\": 9,\n",
        "    \"DoS attacks-Slowloris\": 10,\n",
        "    \"FTP-BruteForce\": 11,\n",
        "    \"Infilteration\": 12,\n",
        "    \"SQL Injection\": 13,\n",
        "    \"SSH-Bruteforce\": 14\n",
        "}\n",
        "\n",
        "## è®¡åˆ’å°è¯•ä¸‰ç§æ ·æœ¬åˆ†å¸ƒæ¨¡å¼\n",
        "resample_schemes = {\n",
        "    # æ¨¡å¼1. (æ ‡ç­¾0:é0æ ‡ç­¾æ€»å’Œ) â‰ˆ (160:157); é0æ ‡ç­¾æŒ‰å¤§æ¦‚æ¯”ä¾‹å¢å¼º\n",
        "    1: {\n",
        "        0: 1600000,  # ä¿æŒä¸å˜\n",
        "        1: 200000,   # â¤µï¸ 228953\n",
        "        2: 20000,    # â¤´ï¸ 489\n",
        "        3: 20000,    # â¤´ï¸ 184\n",
        "        4: 200000,   # â¤µï¸ 548809\n",
        "        5: 20000,    # â¤´ï¸ 1384\n",
        "        6: 200000,   # â¤µï¸ 460953\n",
        "        7: 100000,   # â¤´ï¸ 33206\n",
        "        8: 200000,   # â¤µï¸ 369530\n",
        "        9: 111912,   # â¤´ï¸ 111912\n",
        "        10: 50000,   # â¤´ï¸ 8792\n",
        "        11: 154683,  # â¤´ï¸ 154683\n",
        "        12: 128511,  # â¤´ï¸ 128511\n",
        "        13: 20000,   # â¤´ï¸ 70\n",
        "        14: 150071   # â¤´ï¸ 150071\n",
        "    },\n",
        "    # æ¨¡å¼2. (æ ‡ç­¾0:æœ€å¤šé0æ ‡ç­¾æ ·æœ¬) â‰ˆ (3:2)\n",
        "    2: {\n",
        "        0: 300000,   # â¤µï¸ 1600000\n",
        "        1: 200000,   # â¤µï¸ 228953\n",
        "        2: 20000,    # â¤´ï¸ 489\n",
        "        3: 20000,    # â¤´ï¸ 184\n",
        "        4: 200000,   # â¤µï¸ 548809\n",
        "        5: 20000,    # â¤´ï¸ 1384\n",
        "        6: 200000,   # â¤µï¸ 460953\n",
        "        7: 100000,   # â¤´ï¸ 33206\n",
        "        8: 200000,   # â¤µï¸ 369530\n",
        "        9: 111912,   # â¤´ï¸ 111912\n",
        "        10: 50000,   # â¤´ï¸ 8792\n",
        "        11: 154683,  # â¤´ï¸ 154683\n",
        "        12: 128511,  # â¤´ï¸ 128511\n",
        "        13: 20000,   # â¤´ï¸ 70\n",
        "        14: 150071   # â¤´ï¸ 150071\n",
        "    },\n",
        "    # æ¨¡å¼3. (æ ‡ç­¾0:é0æ ‡ç­¾æ€»å’Œ) = (160:160); æ¯ç§é0æ ‡ç­¾éƒ½å  114300 ä¸ªæ ·æœ¬\n",
        "    3: {\n",
        "        0: 1600000,\n",
        "        **{k: 114300 for k in range(1, 15)}\n",
        "    },\n",
        "    # æ¨¡å¼4. æ‰€æœ‰æ ‡ç­¾éƒ½ 20ä¸‡æ ·æœ¬\n",
        "    4: {\n",
        "       **{k: 200000 for k in range(0, 15)}\n",
        "    },\n",
        "}\n",
        "\n",
        "## æ•°æ®ç›®å½•\n",
        "datasets_folder = Path('/content/drive/MyDrive/NYIT/870/datasets')\n",
        "dataset = 'CSE-CIC-IDS2018'\n",
        "preprocessed_folder = datasets_folder / 'preprocessed' / dataset\n",
        "balanced_folder = datasets_folder / 'balanced' / dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-SqflxGQ5rK"
      },
      "outputs": [],
      "source": [
        "scaling_method = 'standard'\n",
        "# scaling_method = 'robust'\n",
        "\n",
        "resample_scheme = 2\n",
        "resample_to = resample_schemes[resample_scheme]\n",
        "\n",
        "# oversampling_method = 'cGAN'\n",
        "oversampling_method = 'ROS1+cGAN'\n",
        "# undersampling_method = 'NM'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXZ4E8wJlzLF",
        "outputId": "67e8f96e-d4a3-4efd-810e-e98cb09fc0d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/26/25 22:45:51] train_X_standard.npy shape: (3797547, 70), train_label_standard.npy shape: (3797547,)\n",
            "Labels: {0: 1600000, 1: 228953, 2: 489, 3: 184, 4: 548809, 5: 1384, 6: 460953, 7: 33206, 8: 369530, 9: 111912, 10: 8792, 11: 154683, 12: 128511, 13: 70, 14: 150071}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_file = preprocessed_folder / f'integrated/train_X_{scaling_method}.npy'\n",
        "y_file = preprocessed_folder / f'integrated/train_label_{scaling_method}.npy'\n",
        "\n",
        "# åŠ è½½è®­ç»ƒé›†æ–‡ä»¶\n",
        "X = np.load(X_file)\n",
        "y = np.load(y_file)\n",
        "\n",
        "print(f'[{datetime.now().strftime(\"%x %X\")}] {X_file.name} shape: {X.shape}, {y_file.name} shape: {y.shape}')\n",
        "print(f'Labels: { {int(k): v for k, v in sorted(Counter(y).items())} }\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## åˆ©ç”¨ ROS æå‰è¡¥å……æå°‘æ•°ç±»æ ·æœ¬\n",
        "- å…ˆç”¨ ROS éšæœºå¤åˆ¶çš„æ–¹å¼ï¼Œå°†æå°‘æ•°ç±»æ ·æœ¬æ‰©å±•åˆ°å¯æ¥å—çš„ç¨‹åº¦åå†è¿›è¡Œ oversampling\n"
      ],
      "metadata": {
        "id": "fDAkD88lY-gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "oversample_to = {}\n",
        "# åˆ¤æ–­ oversampling_method å­—ç¬¦ä¸²å¼€å¤´æ˜¯å¦ä¸º ROS\n",
        "if oversampling_method.startswith('ROS'):\n",
        "    if oversampling_method.startswith('ROS+'):\n",
        "        oversample_to = {2: 1000, 3: 500, 13: 500}\n",
        "    elif oversampling_method.startswith('ROS1+'):\n",
        "        oversample_to = {2: 1000, 3: 1000, 13: 1000}\n",
        "\n",
        "    oversampler = RandomOverSampler(sampling_strategy=oversample_to, random_state=42)\n",
        "    X, y = oversampler.fit_resample(X, y)\n",
        "\n",
        "    print(f'[{datetime.now().strftime(\"%x %X\")}] After ROS oversampling:')\n",
        "    print(f'  X.shape: {X.shape}, y.shape: {y.shape}')\n",
        "    print(f'  Labels: { {int(k): v for k, v in sorted(Counter(y).items())} }\\n')\n",
        "else:\n",
        "    print(f'[{datetime.now().strftime(\"%x %X\")}] No need to ROS oversampling.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfzi08QuZwQV",
        "outputId": "0af90a6c-c35a-47ed-d124-b70a15a07966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/26/25 22:45:54] After ROS oversampling:\n",
            "  X.shape: (3799804, 70), y.shape: (3799804,)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 1000, 3: 1000, 4: 548809, 5: 1384, 6: 460953, 7: 33206, 8: 369530, 9: 111912, 10: 8792, 11: 154683, 12: 128511, 13: 1000, 14: 150071}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75awp9PSfBmN"
      },
      "source": [
        "## cGAN (conditional Generative Adversarial Network, æ¡ä»¶ç”Ÿæˆå¼å¯¹æŠ—ç½‘ç»œ)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### å®šä¹‰ cGAN"
      ],
      "metadata": {
        "id": "lUwtAak0tyuJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySil5tAjiOvy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# 1ï¸âƒ£ å®šä¹‰ cGAN ç”Ÿæˆå™¨\n",
        "def build_generator(noise_dim, feature_dim, num_classes):\n",
        "    \"\"\"\n",
        "    Builds the generator model for a conditional GAN.\n",
        "\n",
        "    Args:\n",
        "        noise_dim (int): Dimension of the noise input. å™ªå£°å‘é‡çš„ç»´åº¦\n",
        "        feature_dim (int): Dimension of the generated features. ç”Ÿæˆæ•°æ®çš„ç‰¹å¾ç»´åº¦\n",
        "        num_classes (int): Number of classes for the conditional input. ç±»åˆ«æ•°é‡(labels ç§ç±»)\n",
        "\n",
        "    Returns:\n",
        "        Model: A Keras Model representing the generator.\n",
        "    \"\"\"\n",
        "    # å®šä¹‰è¾“å…¥å±‚\n",
        "    noise_input = layers.Input(shape=(noise_dim,))  # å™ªå£°è¾“å…¥ï¼Œè¾“å…¥ä¸€ä¸ª noise_dim ç»´çš„å‘é‡ä½œä¸ºå™ªå£°\n",
        "    label_input = layers.Input(shape=(1,), dtype='int32')  # ç±»åˆ«è¾“å…¥, è¾“å…¥ä¸€ä¸ªæ•´æ•°ä½œä¸ºç±»åˆ«æ ‡ç­¾\n",
        "\n",
        "    # ç±»åˆ«åµŒå…¥å±‚\n",
        "    label_embedding = layers.Embedding(num_classes, noise_dim)(label_input)  # å°†ç±»åˆ«æ ‡ç­¾æ˜ å°„ä¸ºåµŒå…¥å‘é‡(noise_dim ç»´)\n",
        "    label_embedding = layers.Flatten()(label_embedding) # å°†åµŒå…¥å‘é‡å±•å¹³, (1, noise_dim) -> (noise_dim,)\n",
        "\n",
        "    # åˆå¹¶ å™ªå£°å‘é‡ å’Œ ç±»åˆ«åµŒå…¥å‘é‡\n",
        "    # åˆå¹¶åçš„å‘é‡ç»´åº¦ä¸º (noise_dim + noise_dim,) = (2 * noise_dim,)\n",
        "    combined_input = layers.Concatenate()([noise_input, label_embedding])\n",
        "\n",
        "    # ç”Ÿæˆå™¨ç½‘ç»œç»“æ„ (ä¸‰å±‚å…¨è¿æ¥å±‚ Dense, æ¯å±‚ç¥ç»å…ƒæ•°åˆ†åˆ«ä¸º 256, 512, 1024, æ¿€æ´»å‡½æ•°ä¸º relu)\n",
        "    x = layers.Dense(256, activation='relu')(combined_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(1024, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # è¾“å‡ºå±‚ (feature_dim ç»´çš„ç‰¹å¾å‘é‡)\n",
        "    output = layers.Dense(feature_dim, activation='linear')(x)\n",
        "\n",
        "    # è¿”å›ç”Ÿæˆå™¨æ¨¡å‹\n",
        "    # è¯¥æ¨¡å‹æ¥å—ä¸¤ä¸ªè¾“å…¥ï¼Œä¸€ä¸ªæ˜¯å™ªå£°å‘é‡ï¼Œä¸€ä¸ªæ˜¯ç±»åˆ«æ ‡ç­¾. è¾“å‡ºç”Ÿæˆçš„ç‰¹å¾å‘é‡\n",
        "    return Model([noise_input, label_input], output, name=\"Generator\")\n",
        "\n",
        "# 2ï¸âƒ£ å®šä¹‰ cGAN åˆ¤åˆ«å™¨\n",
        "def build_discriminator(feature_dim, num_classes):\n",
        "    \"\"\"\n",
        "    Builds the discriminator model for a conditional GAN.\n",
        "\n",
        "    Args:\n",
        "        feature_dim (int): Dimension of the input features.\n",
        "        num_classes (int): Number of classes for the conditional input.\n",
        "\n",
        "    Returns:\n",
        "        Model: A Keras Model representing the discriminator.\n",
        "    \"\"\"\n",
        "    # å®šä¹‰è¾“å…¥å±‚\n",
        "    data_input = layers.Input(shape=(feature_dim,))  # è¾“å…¥ä¸€ä¸ªç‰¹å¾å‘é‡ (feature_dim ç»´)\n",
        "    label_input = layers.Input(shape=(1,), dtype='int32')  # è¾“å…¥ä¸€ä¸ªç±»åˆ«æ ‡ç­¾ (æ•´æ•°)\n",
        "\n",
        "    # ç±»åˆ«åµŒå…¥å±‚\n",
        "    label_embedding = layers.Embedding(num_classes, feature_dim)(label_input) # å°†ç±»åˆ«æ ‡ç­¾æ˜ å°„ä¸ºåµŒå…¥å‘é‡(feature_dim ç»´)\n",
        "    label_embedding = layers.Flatten()(label_embedding)  # å°†åµŒå…¥å‘é‡å±•å¹³, (1, feature_dim) -> (feature_dim,)\n",
        "\n",
        "    # åˆå¹¶ ç‰¹å¾å‘é‡ å’Œ ç±»åˆ«åµŒå…¥å‘é‡\n",
        "    # åˆå¹¶åçš„å‘é‡ç»´åº¦ä¸º (feature_dim + feature_dim,) = (2 * feature_dim,)\n",
        "    combined_input = layers.Concatenate()([data_input, label_embedding])\n",
        "\n",
        "    # åˆ¤åˆ«å™¨ç½‘ç»œç»“æ„ (ä¸¤å±‚å…¨è¿æ¥å±‚ Dense, æ¯å±‚ç¥ç»å…ƒæ•°åˆ†åˆ«ä¸º 512, 256, æ¿€æ´»å‡½æ•°ä¸º LeakyReLU)\n",
        "    x = layers.Dense(512)(combined_input)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(256)(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "\n",
        "    # è¾“å‡ºå±‚ (sigmoid æ¿€æ´»å‡½æ•°. è¾“å‡ºä¸º [0, 1] åŒºé—´, æ¥è¿‘0: åˆ¤æ–­ä¸ºå‡æ•°æ®, æ¥è¿‘1: åˆ¤æ–­ä¸ºçœŸå®æ•°æ®)\n",
        "    output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # è¿”å›åˆ¤åˆ«å™¨æ¨¡å‹\n",
        "    # è¯¥æ¨¡å‹æ¥å—ä¸¤ä¸ªè¾“å…¥ï¼Œä¸€ä¸ªæ˜¯ç‰¹å¾å‘é‡ï¼Œä¸€ä¸ªæ˜¯ç±»åˆ«æ ‡ç­¾. è¾“å‡ºåˆ¤åˆ«ç»“æœ(çœŸ/å‡ çš„æ¦‚ç‡)\n",
        "    return Model([data_input, label_input], output, name=\"Discriminator\")\n",
        "\n",
        "# 3ï¸âƒ£ ç»„åˆ cGAN\n",
        "def build_gan(generator, discriminator):\n",
        "    \"\"\"\n",
        "    Combines the generator and discriminator to build the conditional GAN (cGAN) model.\n",
        "\n",
        "    Args:\n",
        "        generator (Model): The generator model that generates data based on noise and labels.\n",
        "        discriminator (Model): The discriminator model that evaluates the authenticity of the generated data.\n",
        "\n",
        "    Returns:\n",
        "        Model: A Keras Model representing the combined cGAN, which takes noise and labels as input\n",
        "               and outputs the validity score.\n",
        "    \"\"\"\n",
        "    # å†»ç»“åˆ¤åˆ«å™¨çš„å‚æ•° (åœ¨è®­ç»ƒ cGAN æ—¶ä¸è®­ç»ƒåˆ¤åˆ«å™¨, åªè®­ç»ƒç”Ÿæˆå™¨)\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    noise_dim = generator.input_shape[0][1]\n",
        "\n",
        "    # å®šä¹‰è¾“å…¥å±‚\n",
        "    noise_input = layers.Input(shape=(noise_dim,))\n",
        "    label_input = layers.Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    # ä½¿ç”¨ç”Ÿæˆå™¨ç”Ÿæˆæ•°æ®\n",
        "    generated_data = generator([noise_input, label_input])\n",
        "    # ä½¿ç”¨åˆ¤åˆ«å™¨åˆ¤æ–­ç”Ÿæˆæ•°æ®çš„çœŸä¼ª\n",
        "    validity = discriminator([generated_data, label_input])\n",
        "\n",
        "    # è¿”å› cGAN æ¨¡å‹\n",
        "    # è¯¥æ¨¡å‹æ¥å—ä¸¤ä¸ªè¾“å…¥ï¼Œä¸€ä¸ªæ˜¯å™ªå£°å‘é‡ï¼Œä¸€ä¸ªæ˜¯ç±»åˆ«æ ‡ç­¾. è¾“å‡ºç”Ÿæˆæ•°æ®çš„çœŸå®æ€§è¯„åˆ†\n",
        "    return Model([noise_input, label_input], validity, name=\"cGAN\")\n",
        "\n",
        "# 4ï¸âƒ£ è®­ç»ƒå‡½æ•°\n",
        "def train_cgan(X_train, y_train, generator, discriminator, gan, noise_dim, num_classes, epochs=2000, batch_size=512):\n",
        "    \"\"\"\n",
        "    Trains the conditional GAN (cGAN) model.\n",
        "    ç”Ÿæˆå™¨ generator ç”Ÿæˆæ•°æ®ï¼Œåˆ¤åˆ«å™¨ discriminator åˆ¤æ–­çœŸå‡ï¼Œé€šè¿‡äºŒè€…çš„å¯¹æŠ—è®­ç»ƒï¼Œä¼˜åŒ–ç”Ÿæˆå™¨çš„èƒ½åŠ›ã€‚\n",
        "\n",
        "    Args:\n",
        "        X_train (numpy.ndarray): Training data features.\n",
        "        y_train (numpy.ndarray): Training data labels.\n",
        "        generator (tensorflow.keras.Model): The generator model.\n",
        "        discriminator (tensorflow.keras.Model): The discriminator model.\n",
        "        gan (tensorflow.keras.Model): The combined cGAN model.\n",
        "        noise_dim (int): Dimension of the noise input.\n",
        "        num_classes (int): Number of classes for the conditional input.\n",
        "        epochs (int, optional): Number of training epochs. Defaults to 2000.\n",
        "        batch_size (int, optional): Size of each training batch. Defaults to 512.\n",
        "    \"\"\"\n",
        "    start_time = datetime.now()\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] Training cGAN for {epochs} epochs with batch size {batch_size}...\")\n",
        "\n",
        "    valid = np.ones((batch_size, 1))  # å®šä¹‰çœŸå®æ•°æ®æ ‡ç­¾ (batch_size ä¸ª 1)\n",
        "    fake = np.zeros((batch_size, 1))  # å®šä¹‰ç”Ÿæˆæ•°æ®æ ‡ç­¾ (batch_size ä¸ª 0)\n",
        "\n",
        "    for epoch in range(epochs + 1):\n",
        "        # ä» X_train, y_train ä¸­éšæœºé€‰æ‹©ä¸€ç»„ batch_size çš„çœŸå®æ•°æ®\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        real_samples, real_labels = X_train[idx], y_train[idx]\n",
        "        # ä¿è¯ real_labels çš„ shape æ˜¯ (batch_size, 1) è€Œä¸æ˜¯ (batch_size)\n",
        "        real_labels = real_labels.reshape(-1, 1)\n",
        "\n",
        "        # éšæœºç”Ÿæˆä¸€ç»„å™ªå£°å‘é‡ shape=(batch_size, noise_dim)\n",
        "        noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
        "        # éšæœºç”Ÿæˆä¸€ç»„ç±»åˆ«æ ‡ç­¾ shape=(batch_size, 1)\n",
        "        gen_labels = np.random.randint(0, num_classes, (batch_size, 1))\n",
        "        # ä½¿ç”¨ç”Ÿæˆå™¨ç”Ÿæˆä¸€ç»„ç‰¹å¾å‘é‡ shape=(batch_size, feature_dim)\n",
        "        gen_samples = generator.predict([noise, gen_labels], verbose=0)\n",
        "\n",
        "        ## è®­ç»ƒåˆ¤åˆ«å™¨\n",
        "        # ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒåˆ¤åˆ«å™¨, è¿”å›åˆ¤åˆ«å™¨å¯¹äºçœŸå®æ•°æ®çš„æŸå¤±\n",
        "        d_loss_real = discriminator.train_on_batch([real_samples, real_labels], valid)\n",
        "        # ä½¿ç”¨ç”Ÿæˆæ•°æ®è®­ç»ƒåˆ¤åˆ«å™¨ï¼Œè¿”å›åˆ¤åˆ«å™¨å¯¹äºç”Ÿæˆæ•°æ®çš„æŸå¤±\n",
        "        d_loss_fake = discriminator.train_on_batch([gen_samples, gen_labels], fake)\n",
        "\n",
        "        ## è®­ç»ƒç”Ÿæˆå™¨\n",
        "        # é‡æ–°ç”Ÿæˆä¸€ç»„å™ªå£°å‘é‡ shape=(batch_size, noise_dim) ä¸ ç±»åˆ«æ ‡ç­¾ shape=(batch_size, 1)\n",
        "        noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
        "        sampled_labels = np.random.randint(0, num_classes, (batch_size, 1))\n",
        "        # è®­ç»ƒ cGAN, è¿”å›ç”Ÿæˆå™¨çš„æŸå¤±\n",
        "        g_loss = gan.train_on_batch([noise, sampled_labels], valid)\n",
        "\n",
        "        # æ¯ 500 ä¸ª epoch æ‰“å°ä¸€æ¬¡è¿›åº¦ï¼Œè¾“å‡ºåˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨çš„æŸå¤±\n",
        "        if epoch % 500 == 0:\n",
        "            print(f\"[{datetime.now().strftime('%x %X')}] Epoch {epoch}: Discriminator loss={d_loss_real:.4f}, Generator loss={g_loss:.4f}\")\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    print(f\"[{end_time.strftime('%x %X')}] Training cGAN complete. Time elapsed: {end_time - start_time}\")\n",
        "    pass\n",
        "\n",
        "# 5ï¸âƒ£ ç”Ÿæˆå™¨ç”Ÿæˆå‡½æ•°\n",
        "def generate_samples(generator, target_class, num_samples):\n",
        "    \"\"\"\n",
        "    Generates samples using the generator for a specific target class.\n",
        "\n",
        "    Args:\n",
        "        generator (tensorflow.keras.Model): The generator model.\n",
        "        target_class (int): The target class for which to generate samples.\n",
        "        num_samples (int): The number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Generated samples as a NumPy array.\n",
        "    \"\"\"\n",
        "    noise_dim = generator.input_shape[0][1]\n",
        "\n",
        "    # éšæœºç”Ÿæˆä¸€ç»„å™ªå£°å‘é‡ shape=(num_samples, noise_dim)\n",
        "    noise = np.random.normal(0, 1, (num_samples, noise_dim))\n",
        "    # éšæœºç”Ÿæˆä¸€ç»„ç±»åˆ«æ ‡ç­¾ shape=(num_samples, 1), å…¨éƒ¨ä¸º target_class\n",
        "    labels = np.full((num_samples, 1), target_class)\n",
        "    # ä½¿ç”¨ç”Ÿæˆå™¨ç”Ÿæˆæ•°æ®\n",
        "    generated_data = generator.predict([noise, labels], verbose=0)\n",
        "    return generated_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### åˆå§‹åŒ– å¹¶ è®­ç»ƒ cGAN"
      ],
      "metadata": {
        "id": "cG0WobNKt5iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 1ï¸âƒ£ å‚æ•°é…ç½®\n",
        "noise_dim = 100\n",
        "feature_dim = X.shape[1]\n",
        "num_classes = len(np.unique(y))\n",
        "\n",
        "forece_train = False  # æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒ cGAN\n",
        "save_dir = balanced_folder / 'models' / f'{scaling_method}_{oversampling_method}_n{noise_dim}_f{feature_dim}'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "generator_file = save_dir / 'generator.keras'\n",
        "\n",
        "if generator_file.exists() and not forece_train:\n",
        "    # å¦‚æœå·²ç»å­˜åœ¨é¢„è®­ç»ƒçš„ç”Ÿæˆå™¨æ¨¡å‹ï¼Œåˆ™ç›´æ¥åŠ è½½\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] ğŸ“¡ Loading pre-trained generator from {generator_file}\")\n",
        "    generator = tf.keras.models.load_model(generator_file)\n",
        "else:\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] ğŸš€ Training cGAN...\")\n",
        "\n",
        "    ## 2ï¸âƒ£ åˆå§‹åŒ– cGAN\n",
        "    generator = build_generator(noise_dim, feature_dim, num_classes)\n",
        "    discriminator = build_discriminator(feature_dim, num_classes)\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5))\n",
        "    gan = build_gan(generator, discriminator)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5))\n",
        "\n",
        "    ## 3ï¸âƒ£ è®­ç»ƒ cGAN\n",
        "    train_cgan(X, y, generator, discriminator, gan, noise_dim, num_classes, epochs=5000, batch_size=1024)\n",
        "\n",
        "    ## 4ï¸âƒ£ ä¿å­˜æ¨¡å‹\n",
        "    generator.save(save_dir / 'generator.keras')\n",
        "    discriminator.save(save_dir / 'discriminator.keras')\n",
        "    gan.save(save_dir / 'cgan.keras')\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] âœ… Saved models to {save_dir}\")  # å…¶å®åªéœ€è¦ä¿å­˜ generator å°±å¤Ÿäº†"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TAUxg7ZEtcC",
        "outputId": "f92ceb18-bb31-4e3d-e181-25d0838c65e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/26/25 22:45:55] ğŸ“¡ Loading pre-trained generator from /content/drive/MyDrive/NYIT/870/datasets/balanced/CSE-CIC-IDS2018/models/standard_ROS1+cGAN_n100_f70/generator.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ä½¿ç”¨ cGAN ç”Ÿæˆæ–°æ ·æœ¬"
      ],
      "metadata": {
        "id": "dFXZWgCjzaj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TEST ##\n",
        "\n",
        "# è·å– noise_dim\n",
        "noise_dim = generator.input_shape[0][1]\n",
        "\n",
        "# è·å– feature_dim\n",
        "feature_dim = generator.output_shape[1]\n",
        "\n",
        "# è·å– num_classes\n",
        "from tensorflow.keras.layers import Embedding\n",
        "num_classes = next((layer.input_dim for layer in generator.layers if isinstance(layer, Embedding)), None)\n",
        "\n",
        "print(f\"[{datetime.now().strftime('%x %X')}] æ‰“å°ç”Ÿæˆå™¨çš„å‚æ•°ä¿¡æ¯\")\n",
        "print(f\"generator info - Noise dim: {noise_dim}, Feature dim: {feature_dim}, Num classes: {num_classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HxW9m87LEyV",
        "outputId": "ee0b3578-a796-42bf-8c8b-bc81b2f7c4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/26/25 22:45:55] æ‰“å°ç”Ÿæˆå™¨çš„å‚æ•°ä¿¡æ¯\n",
            "generator info - Noise dim: 100, Feature dim: 70, Num classes: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oversample_to = {}\n",
        "labels_counts = dict(sorted(Counter(y).items()))\n",
        "print(f'[{datetime.now().strftime(\"%x %X\")}] labels_counts: { {int(k): v for k, v in labels_counts.items()} }\\n')\n",
        "\n",
        "for label, target in resample_to.items():\n",
        "    if labels_counts[label] < resample_to[label]:\n",
        "        oversample_to[label] = target\n",
        "# oversample_to = {5: 20000}\n",
        "print(f'[{datetime.now().strftime(\"%x %X\")}] oversample_to scheme{resample_scheme}: {oversample_to}\\n')\n",
        "\n",
        "# æ€»è®¡æ—¶å¼€å§‹\n",
        "start_time = datetime.now()\n",
        "\n",
        "# å…ˆæå–å‡ºä¸éœ€è¦è¿‡é‡‡æ ·çš„æ•°æ®\n",
        "mask = ~np.isin(y, list(oversample_to.keys()))\n",
        "X_resampled = X[mask]\n",
        "y_resampled = y[mask]\n",
        "print(f'X_resampled.shape(before): {X_resampled.shape}')\n",
        "print(f'Labels(before): { {int(k): v for k, v in sorted(Counter(y_resampled).items())} }\\n')\n",
        "\n",
        "for cls, target_count in oversample_to.items():\n",
        "    st = datetime.now()\n",
        "\n",
        "    current_X = X[y == cls]\n",
        "    current_count = current_X.shape[0]\n",
        "    need = target_count - current_count\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] å¤„ç†æ ‡ç­¾ç±»åˆ«[{cls}]: {current_count} -> {target_count}\")\n",
        "\n",
        "    # ç”Ÿæˆæ–°æ ·æœ¬\n",
        "    generated_X = generate_samples(generator, cls, need)\n",
        "\n",
        "    # åˆå¹¶æ ‡ç­¾ç±»åˆ« cls çš„åŸå§‹æ ·æœ¬ä¸æ–°ç”Ÿæˆæ ·æœ¬\n",
        "    sampled_X = np.vstack([current_X, generated_X])\n",
        "    sampled_y = np.concatenate([np.full(current_count, cls), np.full(need, cls)])\n",
        "\n",
        "    # åˆå¹¶åˆ°æœ€ç»ˆçš„ X_resampled, y_resampled\n",
        "    X_resampled = np.vstack([X_resampled, sampled_X])\n",
        "    y_resampled = np.concatenate([y_resampled, sampled_y])\n",
        "\n",
        "    et = datetime.now()\n",
        "    print(f\"  Time elapsed: {et - st}. [{st.strftime('x %X')} -> {et.strftime('x %X')}]\")\n",
        "    print(f'  X_resampled.shape: {X_resampled.shape}')\n",
        "    print(f'  Labels: { {int(k): v for k, v in sorted(Counter(y_resampled).items())} }\\n')\n",
        "\n",
        "print(f\"[{datetime.now().strftime('%x %X')}] âœ… After oversampling:\")\n",
        "end_time = datetime.now()\n",
        "print(f\"  Time elapsed: {end_time - start_time}. [{start_time.strftime('%x %X')} -> {end_time.strftime('%x %X')}]\")\n",
        "print(f'  X_resampled.shape: {X_resampled.shape}')\n",
        "print(f'  Labels: { {int(k): v for k, v in sorted(Counter(y_resampled).items())} }\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhMRbafIzk9e",
        "outputId": "09e45a3e-8e67-4e98-e348-57ef1af2a46e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/26/25 22:45:56] labels_counts: {0: 1600000, 1: 228953, 2: 1000, 3: 1000, 4: 548809, 5: 1384, 6: 460953, 7: 33206, 8: 369530, 9: 111912, 10: 8792, 11: 154683, 12: 128511, 13: 1000, 14: 150071}\n",
            "\n",
            "[04/26/25 22:45:56] oversample_to scheme2: {2: 20000, 3: 20000, 5: 20000, 7: 100000, 10: 50000, 13: 20000}\n",
            "\n",
            "X_resampled.shape(before): (3753422, 70)\n",
            "Labels(before): {0: 1600000, 1: 228953, 4: 548809, 6: 460953, 8: 369530, 9: 111912, 11: 154683, 12: 128511, 14: 150071}\n",
            "\n",
            "[04/26/25 22:45:57] å¤„ç†æ ‡ç­¾ç±»åˆ«[2]: 1000 -> 20000\n",
            "  Time elapsed: 0:00:03.375864. [x 22:45:57 -> x 22:46:00]\n",
            "  X_resampled.shape: (3773422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 20000, 4: 548809, 6: 460953, 8: 369530, 9: 111912, 11: 154683, 12: 128511, 14: 150071}\n",
            "\n",
            "[04/26/25 22:46:01] å¤„ç†æ ‡ç­¾ç±»åˆ«[3]: 1000 -> 20000\n",
            "  Time elapsed: 0:00:04.632296. [x 22:46:01 -> x 22:46:06]\n",
            "  X_resampled.shape: (3793422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 20000, 3: 20000, 4: 548809, 6: 460953, 8: 369530, 9: 111912, 11: 154683, 12: 128511, 14: 150071}\n",
            "\n",
            "[04/26/25 22:46:06] å¤„ç†æ ‡ç­¾ç±»åˆ«[5]: 1384 -> 20000\n",
            "  Time elapsed: 0:00:02.985617. [x 22:46:06 -> x 22:46:09]\n",
            "  X_resampled.shape: (3813422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 20000, 3: 20000, 4: 548809, 5: 20000, 6: 460953, 8: 369530, 9: 111912, 11: 154683, 12: 128511, 14: 150071}\n",
            "\n",
            "[04/26/25 22:46:10] å¤„ç†æ ‡ç­¾ç±»åˆ«[7]: 33206 -> 100000\n",
            "  Time elapsed: 0:00:21.276756. [x 22:46:10 -> x 22:46:31]\n",
            "  X_resampled.shape: (3913422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 20000, 3: 20000, 4: 548809, 5: 20000, 6: 460953, 7: 100000, 8: 369530, 9: 111912, 11: 154683, 12: 128511, 14: 150071}\n",
            "\n",
            "[04/26/25 22:46:33] å¤„ç†æ ‡ç­¾ç±»åˆ«[10]: 8792 -> 50000\n",
            "  Time elapsed: 0:00:10.994688. [x 22:46:33 -> x 22:46:44]\n",
            "  X_resampled.shape: (3963422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 20000, 3: 20000, 4: 548809, 5: 20000, 6: 460953, 7: 100000, 8: 369530, 9: 111912, 10: 50000, 11: 154683, 12: 128511, 14: 150071}\n",
            "\n",
            "[04/26/25 22:46:45] å¤„ç†æ ‡ç­¾ç±»åˆ«[13]: 1000 -> 20000\n",
            "  Time elapsed: 0:00:06.053771. [x 22:46:45 -> x 22:46:51]\n",
            "  X_resampled.shape: (3983422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 20000, 3: 20000, 4: 548809, 5: 20000, 6: 460953, 7: 100000, 8: 369530, 9: 111912, 10: 50000, 11: 154683, 12: 128511, 13: 20000, 14: 150071}\n",
            "\n",
            "[04/26/25 22:46:52] âœ… After oversampling:\n",
            "  Time elapsed: 0:00:56.014058. [04/26/25 22:45:56 -> 04/26/25 22:46:52]\n",
            "  X_resampled.shape: (3983422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 20000, 3: 20000, 4: 548809, 5: 20000, 6: 460953, 7: 100000, 8: 369530, 9: 111912, 10: 50000, 11: 154683, 12: 128511, 13: 20000, 14: 150071}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahMCdpX0cR5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8524eb9b-2af9-4d23-daf5-996db705fc0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/26/25 22:46:58] âœ… Saved to /content/drive/MyDrive/NYIT/870/datasets/balanced/CSE-CIC-IDS2018/train_X_standard_s2_ROS1+cGAN.npy & train_label_standard_s2_ROS1+cGAN.npy\n"
          ]
        }
      ],
      "source": [
        "# ä¿å­˜æ–‡ä»¶\n",
        "X_resampled_file = balanced_folder / f'{X_file.stem}_s{resample_scheme}_{oversampling_method}.npy'\n",
        "y_resampled_file = balanced_folder / f'{y_file.stem}_s{resample_scheme}_{oversampling_method}.npy'\n",
        "\n",
        "# åˆ¤æ–­å®é™… oversample ä¹‹åçš„æ ‡ç­¾æ ·æœ¬æ•°ï¼Œæœ‰æ²¡æœ‰è¾¾åˆ°ç›®æ ‡æ•°é‡ oversample_to çš„ 95%ï¼Œæ²¡æœ‰çš„è¯ï¼Œè®¾ç½®å˜é‡ incomplete ä¸º true\n",
        "incomplete = False\n",
        "incomplete_ratio = 0.95\n",
        "labels_counts = sorted(Counter(y_resampled).items())\n",
        "labels_counts = dict(labels_counts)\n",
        "for label, target in oversample_to.items():\n",
        "    if labels_counts[label] <= target * incomplete_ratio:\n",
        "        incomplete = True\n",
        "        # æ–‡ä»¶ååé¢æœ‰ä¸ª + å·è¡¨ç¤ºè¿‡é‡‡æ ·ä¸å®Œå…¨ï¼Œéœ€è¦ä½¿ç”¨é¢å¤–çš„ç®€å•è¿‡é‡‡æ ·è¿›è¡Œæ•°æ®è¡¥å…¨\n",
        "        X_resampled_file = X_resampled_file.with_name(X_resampled_file.stem + '+.npy')\n",
        "        y_resampled_file = y_resampled_file.with_name(y_resampled_file.stem + '+.npy')\n",
        "        break\n",
        "\n",
        "np.save(X_resampled_file, X_resampled)\n",
        "np.save(y_resampled_file, y_resampled)\n",
        "\n",
        "print(f\"[{datetime.now().strftime('%x %X')}] âœ… Saved to {X_resampled_file} & {y_resampled_file.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp6vxC9t_q25"
      },
      "source": [
        "## SMOTE è¡¥å…¨æ•°æ®\n",
        "\n",
        "å› ä¸ºåŸºäº **é‚»å±…** æ ·æœ¬çš„è¿‡é‡‡æ ·ç®—æ³•, å¯èƒ½ä¼šå› ä¸ºæ‰¾ä¸åˆ°é‚»å±…è€Œå¯¼è‡´æ— æ³•æ–°å¢æ•°æ®ã€‚<br/>\n",
        "æ‰€ä»¥åœ¨æœ€åç”¨ SMOTE ç®—æ³•è¿›è¡Œå…œåº•, è¡¥å…¨ä¸è¶³ oversample_to ç›®æ ‡çš„æ ·æœ¬ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tAavlt9AcNo",
        "outputId": "28c25427-a728-4e79-fafb-b8c656f9dbec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/26/25 22:46:58] âœ… æ— éœ€è¡¥å…¨æ•°æ®\n"
          ]
        }
      ],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "if not incomplete:\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] âœ… æ— éœ€è¡¥å…¨æ•°æ®\")\n",
        "else:\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] âš ï¸ éœ€è¦è¡¥å…¨æ•°æ®\")\n",
        "\n",
        "    # åŠ è½½è®­ç»ƒé›†æ–‡ä»¶\n",
        "    # X_resampled_file = balanced_folder / f'{X_file.stem}_s{resample_scheme}_BLSMOTE.npy'\n",
        "    # y_resampled_file = balanced_folder / f'{y_file.stem}_s{resample_scheme}_BLSMOTE.npy'\n",
        "\n",
        "    # X_resampled = np.load(X_resampled_file)\n",
        "    # y_resampled = np.load(y_resampled_file)\n",
        "\n",
        "    labels_counts = sorted(Counter(y_resampled).items())\n",
        "    labels_counts = dict(labels_counts)\n",
        "\n",
        "    print(f'X_resampled.shape: {X_resampled.shape}')\n",
        "    print(f'Labels: { {int(k): v for k, v in labels_counts.items()} }\\n')\n",
        "\n",
        "    # æŒ‡å®šéœ€è¦è¡¥å……è¿‡é‡‡æ ·çš„æ ‡ç­¾ä¸ç›®æ ‡\n",
        "    oversample_to = {}\n",
        "    for label, target in resample_to.items():\n",
        "        if labels_counts[label] < resample_to[label]:\n",
        "            oversample_to[label] = target\n",
        "    print(f'[{datetime.now().strftime(\"%x %X\")}] oversample_to: {oversample_to}\\n')\n",
        "\n",
        "    # ä½¿ç”¨ SMOTE è¿‡é‡‡æ ·\n",
        "    sampler = SMOTE(sampling_strategy=oversample_to, random_state=42)\n",
        "    X_completed, y_completed = sampler.fit_resample(X_resampled, y_resampled)\n",
        "\n",
        "    # æ‰“å°ç»“æœ\n",
        "    print(f'  X_completed.shape: {X_completed.shape}')\n",
        "    print(f'  Labels: { {int(k): v for k, v in sorted(Counter(y_completed).items())} }\\n')\n",
        "\n",
        "    # ä¿å­˜ç»“æœ\n",
        "    X_completed_file = X_resampled_file.with_name(X_resampled_file.stem + 'SMOTE.npy')\n",
        "    y_completed_file = y_resampled_file.with_name(y_resampled_file.stem + 'SMOTE.npy')\n",
        "\n",
        "    np.save(X_completed_file, X_completed)\n",
        "    np.save(y_completed_file, y_completed)\n",
        "\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] âœ… Saved to {X_completed_file} & {y_completed_file.name}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gqqi2ITUeOmB"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}