{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/funway/Countdown/blob/master/imbalance%20processing/cGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv4skFSheBzA"
      },
      "source": [
        "# 使用 cGAN 对训练集进行过采样\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqqi2ITUeOmB"
      },
      "source": [
        "## Google Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzbTh_MiPCpH",
        "outputId": "a0b25dc9-6a6f-4d70-bf3a-276b50cebf57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR21AFUgeYXb"
      },
      "source": [
        "## Modules import & Globals setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKNH9UjKeZmU"
      },
      "outputs": [],
      "source": [
        "### Modules ###\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "\n",
        "### Globals ###\n",
        "\n",
        "## Label 特征的数值化编码\n",
        "label_mapping = {\n",
        "    \"Benign\": 0,\n",
        "    \"Bot\": 1,\n",
        "    \"Brute Force -Web\": 2,\n",
        "    \"Brute Force -XSS\": 3,\n",
        "    \"DDOS attack-HOIC\": 4,\n",
        "    \"DDOS attack-LOIC-UDP\": 5,\n",
        "    \"DDoS attacks-LOIC-HTTP\": 6,\n",
        "    \"DoS attacks-GoldenEye\": 7,\n",
        "    \"DoS attacks-Hulk\": 8,\n",
        "    \"DoS attacks-SlowHTTPTest\": 9,\n",
        "    \"DoS attacks-Slowloris\": 10,\n",
        "    \"FTP-BruteForce\": 11,\n",
        "    \"Infilteration\": 12,\n",
        "    \"SQL Injection\": 13,\n",
        "    \"SSH-Bruteforce\": 14\n",
        "}\n",
        "\n",
        "## 计划尝试三种样本分布模式\n",
        "resample_schemes = {\n",
        "    # 模式1. (标签0:非0标签总和) ≈ (160:157); 非0标签按大概比例增强\n",
        "    1: {\n",
        "        0: 1600000,  # 保持不变\n",
        "        1: 200000,   # ⤵️ 228953\n",
        "        2: 20000,    # ⤴️ 489\n",
        "        3: 20000,    # ⤴️ 184\n",
        "        4: 200000,   # ⤵️ 548809\n",
        "        5: 20000,    # ⤴️ 1384\n",
        "        6: 200000,   # ⤵️ 460953\n",
        "        7: 100000,   # ⤴️ 33206\n",
        "        8: 200000,   # ⤵️ 369530\n",
        "        9: 111912,   # ⤴️ 111912\n",
        "        10: 50000,   # ⤴️ 8792\n",
        "        11: 154683,  # ⤴️ 154683\n",
        "        12: 128511,  # ⤴️ 128511\n",
        "        13: 20000,   # ⤴️ 70\n",
        "        14: 150071   # ⤴️ 150071\n",
        "    },\n",
        "    # 模式2. (标签0:最多非0标签样本) ≈ (3:2)\n",
        "    2: {\n",
        "        0: 300000,   # ⤵️ 1600000\n",
        "        1: 200000,   # ⤵️ 228953\n",
        "        2: 20000,    # ⤴️ 489\n",
        "        3: 20000,    # ⤴️ 184\n",
        "        4: 200000,   # ⤵️ 548809\n",
        "        5: 20000,    # ⤴️ 1384\n",
        "        6: 200000,   # ⤵️ 460953\n",
        "        7: 100000,   # ⤴️ 33206\n",
        "        8: 200000,   # ⤵️ 369530\n",
        "        9: 111912,   # ⤴️ 111912\n",
        "        10: 50000,   # ⤴️ 8792\n",
        "        11: 154683,  # ⤴️ 154683\n",
        "        12: 128511,  # ⤴️ 128511\n",
        "        13: 20000,   # ⤴️ 70\n",
        "        14: 150071   # ⤴️ 150071\n",
        "    },\n",
        "    # 模式3. (标签0:非0标签总和) = (160:160); 每种非0标签都占 114300 个样本\n",
        "    3: {\n",
        "        0: 1600000,\n",
        "        **{k: 114300 for k in range(1, 15)}\n",
        "    },\n",
        "    # 模式4. 所有标签都 20万样本\n",
        "    4: {\n",
        "       **{k: 200000 for k in range(0, 15)}\n",
        "    },\n",
        "}\n",
        "\n",
        "## 数据目录\n",
        "datasets_folder = Path('/content/drive/MyDrive/NYIT/870/datasets')\n",
        "dataset = 'CSE-CIC-IDS2018'\n",
        "preprocessed_folder = datasets_folder / 'preprocessed' / dataset\n",
        "balanced_folder = datasets_folder / 'balanced' / dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-SqflxGQ5rK"
      },
      "outputs": [],
      "source": [
        "scaling_method = 'standard'\n",
        "# scaling_method = 'robust'\n",
        "\n",
        "resample_scheme = 2\n",
        "resample_to = resample_schemes[resample_scheme]\n",
        "\n",
        "# oversampling_method = 'cGAN'\n",
        "oversampling_method = 'ROS1+cGAN'\n",
        "# undersampling_method = 'NM'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXZ4E8wJlzLF",
        "outputId": "67e8f96e-d4a3-4efd-810e-e98cb09fc0d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/26/25 22:45:51] train_X_standard.npy shape: (3797547, 70), train_label_standard.npy shape: (3797547,)\n",
            "Labels: {0: 1600000, 1: 228953, 2: 489, 3: 184, 4: 548809, 5: 1384, 6: 460953, 7: 33206, 8: 369530, 9: 111912, 10: 8792, 11: 154683, 12: 128511, 13: 70, 14: 150071}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_file = preprocessed_folder / f'integrated/train_X_{scaling_method}.npy'\n",
        "y_file = preprocessed_folder / f'integrated/train_label_{scaling_method}.npy'\n",
        "\n",
        "# 加载训练集文件\n",
        "X = np.load(X_file)\n",
        "y = np.load(y_file)\n",
        "\n",
        "print(f'[{datetime.now().strftime(\"%x %X\")}] {X_file.name} shape: {X.shape}, {y_file.name} shape: {y.shape}')\n",
        "print(f'Labels: { {int(k): v for k, v in sorted(Counter(y).items())} }\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 利用 ROS 提前补充极少数类样本\n",
        "- 先用 ROS 随机复制的方式，将极少数类样本扩展到可接受的程度后再进行 oversampling\n"
      ],
      "metadata": {
        "id": "fDAkD88lY-gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "oversample_to = {}\n",
        "# 判断 oversampling_method 字符串开头是否为 ROS\n",
        "if oversampling_method.startswith('ROS'):\n",
        "    if oversampling_method.startswith('ROS+'):\n",
        "        oversample_to = {2: 1000, 3: 500, 13: 500}\n",
        "    elif oversampling_method.startswith('ROS1+'):\n",
        "        oversample_to = {2: 1000, 3: 1000, 13: 1000}\n",
        "\n",
        "    oversampler = RandomOverSampler(sampling_strategy=oversample_to, random_state=42)\n",
        "    X, y = oversampler.fit_resample(X, y)\n",
        "\n",
        "    print(f'[{datetime.now().strftime(\"%x %X\")}] After ROS oversampling:')\n",
        "    print(f'  X.shape: {X.shape}, y.shape: {y.shape}')\n",
        "    print(f'  Labels: { {int(k): v for k, v in sorted(Counter(y).items())} }\\n')\n",
        "else:\n",
        "    print(f'[{datetime.now().strftime(\"%x %X\")}] No need to ROS oversampling.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfzi08QuZwQV",
        "outputId": "0af90a6c-c35a-47ed-d124-b70a15a07966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/26/25 22:45:54] After ROS oversampling:\n",
            "  X.shape: (3799804, 70), y.shape: (3799804,)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 1000, 3: 1000, 4: 548809, 5: 1384, 6: 460953, 7: 33206, 8: 369530, 9: 111912, 10: 8792, 11: 154683, 12: 128511, 13: 1000, 14: 150071}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75awp9PSfBmN"
      },
      "source": [
        "## cGAN (conditional Generative Adversarial Network, 条件生成式对抗网络)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 定义 cGAN"
      ],
      "metadata": {
        "id": "lUwtAak0tyuJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySil5tAjiOvy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# 1️⃣ 定义 cGAN 生成器\n",
        "def build_generator(noise_dim, feature_dim, num_classes):\n",
        "    \"\"\"\n",
        "    Builds the generator model for a conditional GAN.\n",
        "\n",
        "    Args:\n",
        "        noise_dim (int): Dimension of the noise input. 噪声向量的维度\n",
        "        feature_dim (int): Dimension of the generated features. 生成数据的特征维度\n",
        "        num_classes (int): Number of classes for the conditional input. 类别数量(labels 种类)\n",
        "\n",
        "    Returns:\n",
        "        Model: A Keras Model representing the generator.\n",
        "    \"\"\"\n",
        "    # 定义输入层\n",
        "    noise_input = layers.Input(shape=(noise_dim,))  # 噪声输入，输入一个 noise_dim 维的向量作为噪声\n",
        "    label_input = layers.Input(shape=(1,), dtype='int32')  # 类别输入, 输入一个整数作为类别标签\n",
        "\n",
        "    # 类别嵌入层\n",
        "    label_embedding = layers.Embedding(num_classes, noise_dim)(label_input)  # 将类别标签映射为嵌入向量(noise_dim 维)\n",
        "    label_embedding = layers.Flatten()(label_embedding) # 将嵌入向量展平, (1, noise_dim) -> (noise_dim,)\n",
        "\n",
        "    # 合并 噪声向量 和 类别嵌入向量\n",
        "    # 合并后的向量维度为 (noise_dim + noise_dim,) = (2 * noise_dim,)\n",
        "    combined_input = layers.Concatenate()([noise_input, label_embedding])\n",
        "\n",
        "    # 生成器网络结构 (三层全连接层 Dense, 每层神经元数分别为 256, 512, 1024, 激活函数为 relu)\n",
        "    x = layers.Dense(256, activation='relu')(combined_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dense(1024, activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # 输出层 (feature_dim 维的特征向量)\n",
        "    output = layers.Dense(feature_dim, activation='linear')(x)\n",
        "\n",
        "    # 返回生成器模型\n",
        "    # 该模型接受两个输入，一个是噪声向量，一个是类别标签. 输出生成的特征向量\n",
        "    return Model([noise_input, label_input], output, name=\"Generator\")\n",
        "\n",
        "# 2️⃣ 定义 cGAN 判别器\n",
        "def build_discriminator(feature_dim, num_classes):\n",
        "    \"\"\"\n",
        "    Builds the discriminator model for a conditional GAN.\n",
        "\n",
        "    Args:\n",
        "        feature_dim (int): Dimension of the input features.\n",
        "        num_classes (int): Number of classes for the conditional input.\n",
        "\n",
        "    Returns:\n",
        "        Model: A Keras Model representing the discriminator.\n",
        "    \"\"\"\n",
        "    # 定义输入层\n",
        "    data_input = layers.Input(shape=(feature_dim,))  # 输入一个特征向量 (feature_dim 维)\n",
        "    label_input = layers.Input(shape=(1,), dtype='int32')  # 输入一个类别标签 (整数)\n",
        "\n",
        "    # 类别嵌入层\n",
        "    label_embedding = layers.Embedding(num_classes, feature_dim)(label_input) # 将类别标签映射为嵌入向量(feature_dim 维)\n",
        "    label_embedding = layers.Flatten()(label_embedding)  # 将嵌入向量展平, (1, feature_dim) -> (feature_dim,)\n",
        "\n",
        "    # 合并 特征向量 和 类别嵌入向量\n",
        "    # 合并后的向量维度为 (feature_dim + feature_dim,) = (2 * feature_dim,)\n",
        "    combined_input = layers.Concatenate()([data_input, label_embedding])\n",
        "\n",
        "    # 判别器网络结构 (两层全连接层 Dense, 每层神经元数分别为 512, 256, 激活函数为 LeakyReLU)\n",
        "    x = layers.Dense(512)(combined_input)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "    x = layers.Dense(256)(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    x = layers.Dropout(0.4)(x)\n",
        "\n",
        "    # 输出层 (sigmoid 激活函数. 输出为 [0, 1] 区间, 接近0: 判断为假数据, 接近1: 判断为真实数据)\n",
        "    output = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # 返回判别器模型\n",
        "    # 该模型接受两个输入，一个是特征向量，一个是类别标签. 输出判别结果(真/假 的概率)\n",
        "    return Model([data_input, label_input], output, name=\"Discriminator\")\n",
        "\n",
        "# 3️⃣ 组合 cGAN\n",
        "def build_gan(generator, discriminator):\n",
        "    \"\"\"\n",
        "    Combines the generator and discriminator to build the conditional GAN (cGAN) model.\n",
        "\n",
        "    Args:\n",
        "        generator (Model): The generator model that generates data based on noise and labels.\n",
        "        discriminator (Model): The discriminator model that evaluates the authenticity of the generated data.\n",
        "\n",
        "    Returns:\n",
        "        Model: A Keras Model representing the combined cGAN, which takes noise and labels as input\n",
        "               and outputs the validity score.\n",
        "    \"\"\"\n",
        "    # 冻结判别器的参数 (在训练 cGAN 时不训练判别器, 只训练生成器)\n",
        "    discriminator.trainable = False\n",
        "\n",
        "    noise_dim = generator.input_shape[0][1]\n",
        "\n",
        "    # 定义输入层\n",
        "    noise_input = layers.Input(shape=(noise_dim,))\n",
        "    label_input = layers.Input(shape=(1,), dtype='int32')\n",
        "\n",
        "    # 使用生成器生成数据\n",
        "    generated_data = generator([noise_input, label_input])\n",
        "    # 使用判别器判断生成数据的真伪\n",
        "    validity = discriminator([generated_data, label_input])\n",
        "\n",
        "    # 返回 cGAN 模型\n",
        "    # 该模型接受两个输入，一个是噪声向量，一个是类别标签. 输出生成数据的真实性评分\n",
        "    return Model([noise_input, label_input], validity, name=\"cGAN\")\n",
        "\n",
        "# 4️⃣ 训练函数\n",
        "def train_cgan(X_train, y_train, generator, discriminator, gan, noise_dim, num_classes, epochs=2000, batch_size=512):\n",
        "    \"\"\"\n",
        "    Trains the conditional GAN (cGAN) model.\n",
        "    生成器 generator 生成数据，判别器 discriminator 判断真假，通过二者的对抗训练，优化生成器的能力。\n",
        "\n",
        "    Args:\n",
        "        X_train (numpy.ndarray): Training data features.\n",
        "        y_train (numpy.ndarray): Training data labels.\n",
        "        generator (tensorflow.keras.Model): The generator model.\n",
        "        discriminator (tensorflow.keras.Model): The discriminator model.\n",
        "        gan (tensorflow.keras.Model): The combined cGAN model.\n",
        "        noise_dim (int): Dimension of the noise input.\n",
        "        num_classes (int): Number of classes for the conditional input.\n",
        "        epochs (int, optional): Number of training epochs. Defaults to 2000.\n",
        "        batch_size (int, optional): Size of each training batch. Defaults to 512.\n",
        "    \"\"\"\n",
        "    start_time = datetime.now()\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] Training cGAN for {epochs} epochs with batch size {batch_size}...\")\n",
        "\n",
        "    valid = np.ones((batch_size, 1))  # 定义真实数据标签 (batch_size 个 1)\n",
        "    fake = np.zeros((batch_size, 1))  # 定义生成数据标签 (batch_size 个 0)\n",
        "\n",
        "    for epoch in range(epochs + 1):\n",
        "        # 从 X_train, y_train 中随机选择一组 batch_size 的真实数据\n",
        "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "        real_samples, real_labels = X_train[idx], y_train[idx]\n",
        "        # 保证 real_labels 的 shape 是 (batch_size, 1) 而不是 (batch_size)\n",
        "        real_labels = real_labels.reshape(-1, 1)\n",
        "\n",
        "        # 随机生成一组噪声向量 shape=(batch_size, noise_dim)\n",
        "        noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
        "        # 随机生成一组类别标签 shape=(batch_size, 1)\n",
        "        gen_labels = np.random.randint(0, num_classes, (batch_size, 1))\n",
        "        # 使用生成器生成一组特征向量 shape=(batch_size, feature_dim)\n",
        "        gen_samples = generator.predict([noise, gen_labels], verbose=0)\n",
        "\n",
        "        ## 训练判别器\n",
        "        # 使用真实数据训练判别器, 返回判别器对于真实数据的损失\n",
        "        d_loss_real = discriminator.train_on_batch([real_samples, real_labels], valid)\n",
        "        # 使用生成数据训练判别器，返回判别器对于生成数据的损失\n",
        "        d_loss_fake = discriminator.train_on_batch([gen_samples, gen_labels], fake)\n",
        "\n",
        "        ## 训练生成器\n",
        "        # 重新生成一组噪声向量 shape=(batch_size, noise_dim) 与 类别标签 shape=(batch_size, 1)\n",
        "        noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
        "        sampled_labels = np.random.randint(0, num_classes, (batch_size, 1))\n",
        "        # 训练 cGAN, 返回生成器的损失\n",
        "        g_loss = gan.train_on_batch([noise, sampled_labels], valid)\n",
        "\n",
        "        # 每 500 个 epoch 打印一次进度，输出判别器和生成器的损失\n",
        "        if epoch % 500 == 0:\n",
        "            print(f\"[{datetime.now().strftime('%x %X')}] Epoch {epoch}: Discriminator loss={d_loss_real:.4f}, Generator loss={g_loss:.4f}\")\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    print(f\"[{end_time.strftime('%x %X')}] Training cGAN complete. Time elapsed: {end_time - start_time}\")\n",
        "    pass\n",
        "\n",
        "# 5️⃣ 生成器生成函数\n",
        "def generate_samples(generator, target_class, num_samples):\n",
        "    \"\"\"\n",
        "    Generates samples using the generator for a specific target class.\n",
        "\n",
        "    Args:\n",
        "        generator (tensorflow.keras.Model): The generator model.\n",
        "        target_class (int): The target class for which to generate samples.\n",
        "        num_samples (int): The number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Generated samples as a NumPy array.\n",
        "    \"\"\"\n",
        "    noise_dim = generator.input_shape[0][1]\n",
        "\n",
        "    # 随机生成一组噪声向量 shape=(num_samples, noise_dim)\n",
        "    noise = np.random.normal(0, 1, (num_samples, noise_dim))\n",
        "    # 随机生成一组类别标签 shape=(num_samples, 1), 全部为 target_class\n",
        "    labels = np.full((num_samples, 1), target_class)\n",
        "    # 使用生成器生成数据\n",
        "    generated_data = generator.predict([noise, labels], verbose=0)\n",
        "    return generated_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 初始化 并 训练 cGAN"
      ],
      "metadata": {
        "id": "cG0WobNKt5iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 1️⃣ 参数配置\n",
        "noise_dim = 100\n",
        "feature_dim = X.shape[1]\n",
        "num_classes = len(np.unique(y))\n",
        "\n",
        "forece_train = False  # 是否强制重新训练 cGAN\n",
        "save_dir = balanced_folder / 'models' / f'{scaling_method}_{oversampling_method}_n{noise_dim}_f{feature_dim}'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "generator_file = save_dir / 'generator.keras'\n",
        "\n",
        "if generator_file.exists() and not forece_train:\n",
        "    # 如果已经存在预训练的生成器模型，则直接加载\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] 📡 Loading pre-trained generator from {generator_file}\")\n",
        "    generator = tf.keras.models.load_model(generator_file)\n",
        "else:\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] 🚀 Training cGAN...\")\n",
        "\n",
        "    ## 2️⃣ 初始化 cGAN\n",
        "    generator = build_generator(noise_dim, feature_dim, num_classes)\n",
        "    discriminator = build_discriminator(feature_dim, num_classes)\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5))\n",
        "    gan = build_gan(generator, discriminator)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.0002, 0.5))\n",
        "\n",
        "    ## 3️⃣ 训练 cGAN\n",
        "    train_cgan(X, y, generator, discriminator, gan, noise_dim, num_classes, epochs=5000, batch_size=1024)\n",
        "\n",
        "    ## 4️⃣ 保存模型\n",
        "    generator.save(save_dir / 'generator.keras')\n",
        "    discriminator.save(save_dir / 'discriminator.keras')\n",
        "    gan.save(save_dir / 'cgan.keras')\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] ✅ Saved models to {save_dir}\")  # 其实只需要保存 generator 就够了"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TAUxg7ZEtcC",
        "outputId": "f92ceb18-bb31-4e3d-e181-25d0838c65e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/26/25 22:45:55] 📡 Loading pre-trained generator from /content/drive/MyDrive/NYIT/870/datasets/balanced/CSE-CIC-IDS2018/models/standard_ROS1+cGAN_n100_f70/generator.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用 cGAN 生成新样本"
      ],
      "metadata": {
        "id": "dFXZWgCjzaj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TEST ##\n",
        "\n",
        "# 获取 noise_dim\n",
        "noise_dim = generator.input_shape[0][1]\n",
        "\n",
        "# 获取 feature_dim\n",
        "feature_dim = generator.output_shape[1]\n",
        "\n",
        "# 获取 num_classes\n",
        "from tensorflow.keras.layers import Embedding\n",
        "num_classes = next((layer.input_dim for layer in generator.layers if isinstance(layer, Embedding)), None)\n",
        "\n",
        "print(f\"[{datetime.now().strftime('%x %X')}] 打印生成器的参数信息\")\n",
        "print(f\"generator info - Noise dim: {noise_dim}, Feature dim: {feature_dim}, Num classes: {num_classes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HxW9m87LEyV",
        "outputId": "ee0b3578-a796-42bf-8c8b-bc81b2f7c4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/26/25 22:45:55] 打印生成器的参数信息\n",
            "generator info - Noise dim: 100, Feature dim: 70, Num classes: 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "oversample_to = {}\n",
        "labels_counts = dict(sorted(Counter(y).items()))\n",
        "print(f'[{datetime.now().strftime(\"%x %X\")}] labels_counts: { {int(k): v for k, v in labels_counts.items()} }\\n')\n",
        "\n",
        "for label, target in resample_to.items():\n",
        "    if labels_counts[label] < resample_to[label]:\n",
        "        oversample_to[label] = target\n",
        "# oversample_to = {5: 20000}\n",
        "print(f'[{datetime.now().strftime(\"%x %X\")}] oversample_to scheme{resample_scheme}: {oversample_to}\\n')\n",
        "\n",
        "# 总计时开始\n",
        "start_time = datetime.now()\n",
        "\n",
        "# 先提取出不需要过采样的数据\n",
        "mask = ~np.isin(y, list(oversample_to.keys()))\n",
        "X_resampled = X[mask]\n",
        "y_resampled = y[mask]\n",
        "print(f'X_resampled.shape(before): {X_resampled.shape}')\n",
        "print(f'Labels(before): { {int(k): v for k, v in sorted(Counter(y_resampled).items())} }\\n')\n",
        "\n",
        "for cls, target_count in oversample_to.items():\n",
        "    st = datetime.now()\n",
        "\n",
        "    current_X = X[y == cls]\n",
        "    current_count = current_X.shape[0]\n",
        "    need = target_count - current_count\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] 处理标签类别[{cls}]: {current_count} -> {target_count}\")\n",
        "\n",
        "    # 生成新样本\n",
        "    generated_X = generate_samples(generator, cls, need)\n",
        "\n",
        "    # 合并标签类别 cls 的原始样本与新生成样本\n",
        "    sampled_X = np.vstack([current_X, generated_X])\n",
        "    sampled_y = np.concatenate([np.full(current_count, cls), np.full(need, cls)])\n",
        "\n",
        "    # 合并到最终的 X_resampled, y_resampled\n",
        "    X_resampled = np.vstack([X_resampled, sampled_X])\n",
        "    y_resampled = np.concatenate([y_resampled, sampled_y])\n",
        "\n",
        "    et = datetime.now()\n",
        "    print(f\"  Time elapsed: {et - st}. [{st.strftime('x %X')} -> {et.strftime('x %X')}]\")\n",
        "    print(f'  X_resampled.shape: {X_resampled.shape}')\n",
        "    print(f'  Labels: { {int(k): v for k, v in sorted(Counter(y_resampled).items())} }\\n')\n",
        "\n",
        "print(f\"[{datetime.now().strftime('%x %X')}] ✅ After oversampling:\")\n",
        "end_time = datetime.now()\n",
        "print(f\"  Time elapsed: {end_time - start_time}. [{start_time.strftime('%x %X')} -> {end_time.strftime('%x %X')}]\")\n",
        "print(f'  X_resampled.shape: {X_resampled.shape}')\n",
        "print(f'  Labels: { {int(k): v for k, v in sorted(Counter(y_resampled).items())} }\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhMRbafIzk9e",
        "outputId": "09e45a3e-8e67-4e98-e348-57ef1af2a46e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/26/25 22:45:56] labels_counts: {0: 1600000, 1: 228953, 2: 1000, 3: 1000, 4: 548809, 5: 1384, 6: 460953, 7: 33206, 8: 369530, 9: 111912, 10: 8792, 11: 154683, 12: 128511, 13: 1000, 14: 150071}\n",
            "\n",
            "[04/26/25 22:45:56] oversample_to scheme2: {2: 20000, 3: 20000, 5: 20000, 7: 100000, 10: 50000, 13: 20000}\n",
            "\n",
            "X_resampled.shape(before): (3753422, 70)\n",
            "Labels(before): {0: 1600000, 1: 228953, 4: 548809, 6: 460953, 8: 369530, 9: 111912, 11: 154683, 12: 128511, 14: 150071}\n",
            "\n",
            "[04/26/25 22:45:57] 处理标签类别[2]: 1000 -> 20000\n",
            "  Time elapsed: 0:00:03.375864. [x 22:45:57 -> x 22:46:00]\n",
            "  X_resampled.shape: (3773422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 20000, 4: 548809, 6: 460953, 8: 369530, 9: 111912, 11: 154683, 12: 128511, 14: 150071}\n",
            "\n",
            "[04/26/25 22:46:01] 处理标签类别[3]: 1000 -> 20000\n",
            "  Time elapsed: 0:00:04.632296. [x 22:46:01 -> x 22:46:06]\n",
            "  X_resampled.shape: (3793422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 20000, 3: 20000, 4: 548809, 6: 460953, 8: 369530, 9: 111912, 11: 154683, 12: 128511, 14: 150071}\n",
            "\n",
            "[04/26/25 22:46:06] 处理标签类别[5]: 1384 -> 20000\n",
            "  Time elapsed: 0:00:02.985617. [x 22:46:06 -> x 22:46:09]\n",
            "  X_resampled.shape: (3813422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 20000, 3: 20000, 4: 548809, 5: 20000, 6: 460953, 8: 369530, 9: 111912, 11: 154683, 12: 128511, 14: 150071}\n",
            "\n",
            "[04/26/25 22:46:10] 处理标签类别[7]: 33206 -> 100000\n",
            "  Time elapsed: 0:00:21.276756. [x 22:46:10 -> x 22:46:31]\n",
            "  X_resampled.shape: (3913422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 20000, 3: 20000, 4: 548809, 5: 20000, 6: 460953, 7: 100000, 8: 369530, 9: 111912, 11: 154683, 12: 128511, 14: 150071}\n",
            "\n",
            "[04/26/25 22:46:33] 处理标签类别[10]: 8792 -> 50000\n",
            "  Time elapsed: 0:00:10.994688. [x 22:46:33 -> x 22:46:44]\n",
            "  X_resampled.shape: (3963422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 20000, 3: 20000, 4: 548809, 5: 20000, 6: 460953, 7: 100000, 8: 369530, 9: 111912, 10: 50000, 11: 154683, 12: 128511, 14: 150071}\n",
            "\n",
            "[04/26/25 22:46:45] 处理标签类别[13]: 1000 -> 20000\n",
            "  Time elapsed: 0:00:06.053771. [x 22:46:45 -> x 22:46:51]\n",
            "  X_resampled.shape: (3983422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 20000, 3: 20000, 4: 548809, 5: 20000, 6: 460953, 7: 100000, 8: 369530, 9: 111912, 10: 50000, 11: 154683, 12: 128511, 13: 20000, 14: 150071}\n",
            "\n",
            "[04/26/25 22:46:52] ✅ After oversampling:\n",
            "  Time elapsed: 0:00:56.014058. [04/26/25 22:45:56 -> 04/26/25 22:46:52]\n",
            "  X_resampled.shape: (3983422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 20000, 3: 20000, 4: 548809, 5: 20000, 6: 460953, 7: 100000, 8: 369530, 9: 111912, 10: 50000, 11: 154683, 12: 128511, 13: 20000, 14: 150071}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahMCdpX0cR5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8524eb9b-2af9-4d23-daf5-996db705fc0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/26/25 22:46:58] ✅ Saved to /content/drive/MyDrive/NYIT/870/datasets/balanced/CSE-CIC-IDS2018/train_X_standard_s2_ROS1+cGAN.npy & train_label_standard_s2_ROS1+cGAN.npy\n"
          ]
        }
      ],
      "source": [
        "# 保存文件\n",
        "X_resampled_file = balanced_folder / f'{X_file.stem}_s{resample_scheme}_{oversampling_method}.npy'\n",
        "y_resampled_file = balanced_folder / f'{y_file.stem}_s{resample_scheme}_{oversampling_method}.npy'\n",
        "\n",
        "# 判断实际 oversample 之后的标签样本数，有没有达到目标数量 oversample_to 的 95%，没有的话，设置变量 incomplete 为 true\n",
        "incomplete = False\n",
        "incomplete_ratio = 0.95\n",
        "labels_counts = sorted(Counter(y_resampled).items())\n",
        "labels_counts = dict(labels_counts)\n",
        "for label, target in oversample_to.items():\n",
        "    if labels_counts[label] <= target * incomplete_ratio:\n",
        "        incomplete = True\n",
        "        # 文件名后面有个 + 号表示过采样不完全，需要使用额外的简单过采样进行数据补全\n",
        "        X_resampled_file = X_resampled_file.with_name(X_resampled_file.stem + '+.npy')\n",
        "        y_resampled_file = y_resampled_file.with_name(y_resampled_file.stem + '+.npy')\n",
        "        break\n",
        "\n",
        "np.save(X_resampled_file, X_resampled)\n",
        "np.save(y_resampled_file, y_resampled)\n",
        "\n",
        "print(f\"[{datetime.now().strftime('%x %X')}] ✅ Saved to {X_resampled_file} & {y_resampled_file.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp6vxC9t_q25"
      },
      "source": [
        "## SMOTE 补全数据\n",
        "\n",
        "因为基于 **邻居** 样本的过采样算法, 可能会因为找不到邻居而导致无法新增数据。<br/>\n",
        "所以在最后用 SMOTE 算法进行兜底, 补全不足 oversample_to 目标的样本。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tAavlt9AcNo",
        "outputId": "28c25427-a728-4e79-fafb-b8c656f9dbec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/26/25 22:46:58] ✅ 无需补全数据\n"
          ]
        }
      ],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "if not incomplete:\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] ✅ 无需补全数据\")\n",
        "else:\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] ⚠️ 需要补全数据\")\n",
        "\n",
        "    # 加载训练集文件\n",
        "    # X_resampled_file = balanced_folder / f'{X_file.stem}_s{resample_scheme}_BLSMOTE.npy'\n",
        "    # y_resampled_file = balanced_folder / f'{y_file.stem}_s{resample_scheme}_BLSMOTE.npy'\n",
        "\n",
        "    # X_resampled = np.load(X_resampled_file)\n",
        "    # y_resampled = np.load(y_resampled_file)\n",
        "\n",
        "    labels_counts = sorted(Counter(y_resampled).items())\n",
        "    labels_counts = dict(labels_counts)\n",
        "\n",
        "    print(f'X_resampled.shape: {X_resampled.shape}')\n",
        "    print(f'Labels: { {int(k): v for k, v in labels_counts.items()} }\\n')\n",
        "\n",
        "    # 指定需要补充过采样的标签与目标\n",
        "    oversample_to = {}\n",
        "    for label, target in resample_to.items():\n",
        "        if labels_counts[label] < resample_to[label]:\n",
        "            oversample_to[label] = target\n",
        "    print(f'[{datetime.now().strftime(\"%x %X\")}] oversample_to: {oversample_to}\\n')\n",
        "\n",
        "    # 使用 SMOTE 过采样\n",
        "    sampler = SMOTE(sampling_strategy=oversample_to, random_state=42)\n",
        "    X_completed, y_completed = sampler.fit_resample(X_resampled, y_resampled)\n",
        "\n",
        "    # 打印结果\n",
        "    print(f'  X_completed.shape: {X_completed.shape}')\n",
        "    print(f'  Labels: { {int(k): v for k, v in sorted(Counter(y_completed).items())} }\\n')\n",
        "\n",
        "    # 保存结果\n",
        "    X_completed_file = X_resampled_file.with_name(X_resampled_file.stem + 'SMOTE.npy')\n",
        "    y_completed_file = y_resampled_file.with_name(y_resampled_file.stem + 'SMOTE.npy')\n",
        "\n",
        "    np.save(X_completed_file, X_completed)\n",
        "    np.save(y_completed_file, y_completed)\n",
        "\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] ✅ Saved to {X_completed_file} & {y_completed_file.name}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gqqi2ITUeOmB"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}